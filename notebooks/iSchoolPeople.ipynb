{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iSchool People and Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import networkx as nx\n",
    "from thefuzz import fuzz\n",
    "import itertools\n",
    "from networkx.algorithms import bipartite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ischool.illinois.edu/research/areas\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "research_links = soup.find_all('li', class_='taxonomy-term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for link in research_links:\n",
    "    term_link = link.find('a', class_='taxonomy-term__link')\n",
    "    term_href = \"https://ischool.illinois.edu\" + term_link.get('href')\n",
    "    term_name = term_link.get_text()\n",
    "    description = link.find('div', class_='taxonomy-term__description').get_text()\n",
    "    dfs.append({'research_area': term_name, 'research_description': description, 'research_area_url': term_href})\n",
    "research_areas = pd.DataFrame(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for _, row in research_areas.iterrows():\n",
    "    response = requests.get(row['research_area_url'])\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    faculty = soup.find_all('li', class_='personnel-list__person-item')\n",
    "    for fac in faculty:\n",
    "        fac_name = fac.find('div', class_='personnel-list__person-name').get_text()\n",
    "        fac_title = fac.find('div', class_='personnel-list__person-role').get_text()\n",
    "        fac_url = \"https://ischool.illinois.edu\" + fac.find('a', class_='personnel-list__person-link').get('href')\n",
    "        data = {'name': fac_name, 'description': fac_title, 'url': fac_url, 'research_area': row['research_area'], 'research_url': row.research_area_url, 'research_description': row.research_description}\n",
    "        dfs.append(data)\n",
    "final_df = pd.DataFrame(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "committees_url = \"https://ischool.illinois.edu/people/committees\"\n",
    "committees_r = requests.get(committees_url)\n",
    "committees_soup = BeautifulSoup(committees_r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = committees_soup.find('div', class_='text-plus-image text-plus-image__noimage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = div.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for p in paragraphs:\n",
    "    title = p.find('strong').get_text() if p.find('strong') else ''\n",
    "    members = p.get_text().replace(title, '').split(',')\n",
    "    data = {}\n",
    "    \n",
    "    for member in members:\n",
    "        \n",
    "        cleaned_member = member.strip().replace('(ex officio)', '').replace('(Chair)', ''). replace('ex officio)', '').replace('(Chair', '')\n",
    "        split_members = cleaned_member.split('\\r\\n')\n",
    "        split_members = [member for member in split_members if len(member) > 1]\n",
    "        split_members = [member for member in split_members if 'chair' not in member.lower()]\n",
    "        for m in split_members:\n",
    "            data = {'committee_title': title, 'committee_member': m}\n",
    "            dfs.append(data)\n",
    "committee_df = pd.DataFrame(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_df['name'] = committee_df.committee_member\n",
    "committee_df.loc[committee_df.committee_title == \"\", \"committee_title\"] = \"Executive Committee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_names = final_df.name.unique()\n",
    "committee_names = committee_df.committee_member.unique()\n",
    "\n",
    "names = list(itertools.product(original_names, committee_names))\n",
    "\n",
    "dfs =[]\n",
    "for name in names:\n",
    "    score = fuzz.token_set_ratio(name[0], name[1])\n",
    "    if score > 90:\n",
    "        data = {'name': name[0], 'committee_member': name[1], 'score': score}\n",
    "        dfs.append(data)\n",
    "\n",
    "matches_df = pd.DataFrame(dfs)\n",
    "matches_df = matches_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(matches_df, committee_df[['committee_member', 'committee_title']], on=['committee_member'], how='outer')\n",
    "\n",
    "merged_df.loc[merged_df.name.isna(), 'name'] = merged_df.committee_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.merge(final_df, merged_df, on=['name'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_csv(\"scraped_ischool_people.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_melted = pd.melt(all_df, id_vars=['name', 'description', 'url', 'research_url', 'research_description', 'committee_member', 'score'], value_vars=['research_area', 'committee_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_nodes = all_melted[['name', 'description', 'url']].drop_duplicates()\n",
    "# groupby name and drop None values\n",
    "\n",
    "subset_people_nodes = people_nodes.groupby('name').agg({'description': 'first', 'url': 'first'}).reset_index()\n",
    "\n",
    "area_nodes = all_melted[['value', 'variable', 'research_description', 'research_url']].drop_duplicates()\n",
    "\n",
    "subset_area_nodes = area_nodes.groupby(['value', 'variable']).agg({'research_description': 'first', 'research_url': 'first'}).reset_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = all_melted[['name', 'value']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes from subset_people_nodes\n",
    "for _, row in subset_people_nodes.iterrows():\n",
    "    attributes = {key: value for key, value in row[['description', 'url']].items() if pd.notnull(value)}\n",
    "    G.add_node(row['name'], bipartite=0, **attributes)\n",
    "\n",
    "# Add nodes from subset_area_nodes\n",
    "for _, row in subset_area_nodes.iterrows():\n",
    "    attributes = {key: value for key, value in row[['variable', 'research_description', 'research_url']].items() if pd.notnull(value)}\n",
    "    G.add_node(row['value'], bipartite=1, **attributes)\n",
    "\n",
    "G.add_edges_from(edges.values)\n",
    "nx.write_gexf(G, \"ischool_people_research_areas.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_nodes, top_nodes = bipartite.sets(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = bipartite.weighted_projected_graph(G, bottom_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(B, \"ischool_people_committees_updated.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_edges(borrow_events, group_col, list_col):\n",
    "    edges = []\n",
    "\n",
    "    def create_edges(rows):\n",
    "        if len(rows[f'list_{list_col}']) > 1:\n",
    "            combos = list(itertools.combinations(rows[f'list_{list_col}'], 2))\n",
    "\n",
    "            for c in combos:\n",
    "                edge = {}\n",
    "                edge['source'] = c[0]\n",
    "                edge['target'] = c[1]\n",
    "                edge[f'{group_col}'] = rows[group_col]\n",
    "                edges.append(pd.DataFrame([edge]))\n",
    "\n",
    "    borrow_events.groupby(f'{group_col}')[f'{list_col}'].apply(list).reset_index(name=f'list_{list_col}').progress_apply(create_edges, axis=1)\n",
    "    final_edges = pd.concat(edges)\n",
    "    grouped_edges = final_edges.groupby(\n",
    "        ['source', 'target', f'{group_col}']).size().reset_index(name='counts')\n",
    "    return grouped_edges\n",
    "\n",
    "def get_attrs(dict_attrs, rows):\n",
    "    updated_dict_attrs = dict_attrs.copy()\n",
    "    for k, v in dict_attrs.items():\n",
    "        updated_dict_attrs[k] = rows[v]\n",
    "    \n",
    "    return updated_dict_attrs\n",
    "\n",
    "def add_nodes(rows, graph, node_attrs):\n",
    "    updated_node_attrs = get_attrs(node_attrs, rows) if len(\n",
    "        node_attrs) > 1 else node_attrs\n",
    "    graph.add_nodes_from(rows, **updated_node_attrs)\n",
    "\n",
    "def add_edges(rows, graph, edge_attrs):\n",
    "    updated_edge_attrs = get_attrs(edge_attrs, rows)\n",
    "    graph.add_edges_from([(rows.source, rows.target)], **updated_edge_attrs)\n",
    "\n",
    "def create_unipartite_network(df, graph, node_attrs, edge_attrs, node_col, edge_col):\n",
    "    '''Create a unipartite graph either members or books'''\n",
    "    nodelist = df.loc[:, [node_col]]\n",
    "    edgelist = build_edges(df, edge_col, node_col)\n",
    "    nodelist.apply(add_nodes, graph=graph, node_attrs=node_attrs, axis=1)\n",
    "    edgelist.apply(add_edges, graph=graph, edge_attrs=edge_attrs, axis=1)\n",
    "\n",
    "def create_bipartite_network(rows, graph, member_attrs, book_attrs, edge_attrs):\n",
    "    \n",
    "    updated_member_attrs = get_attrs(member_attrs, rows)\n",
    "    updated_book_attrs = get_attrs(book_attrs, rows)\n",
    "    updated_edge_attrs = get_attrs(edge_attrs, rows)\n",
    "\n",
    "    tuples = [(rows.member_id, rows.item_uri)]\n",
    "    graph.add_node(rows.member_id, **updated_member_attrs,\n",
    "                   group='members', bipartite=0)\n",
    "    graph.add_node(rows.item_uri, group='books',\n",
    "                   bipartite=1, **updated_book_attrs)\n",
    "    graph.add_edges_from(tuples, **updated_edge_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-work-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
